2025-09-27 23:54:55,099 - __main__ - INFO - === API Starting - Logging Config Applied ===
2025-09-27 23:54:55,273 - app - INFO - === API Starting - Logging Config Applied ===
2025-09-27 23:54:55,285 - app - INFO - üöÄ API Startup
2025-09-27 23:54:55,293 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-09-27 23:54:56,486 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-09-27 23:54:58,052 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-27 23:55:12,835 - app - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-09-27 23:55:12,836 - app - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-09-27 23:55:34,044 - kb_integration - INFO - No KB endpoint configured, using enhanced mock response
2025-09-27 23:57:16,026 - kb_integration - INFO - No KB endpoint configured, using enhanced mock response
2025-09-27 23:57:42,220 - kb_integration - INFO - No KB endpoint configured, using enhanced mock response
2025-09-27 23:57:49,288 - kb_integration - INFO - No KB endpoint configured, using enhanced mock response
2025-09-27 23:58:10,334 - kb_integration - INFO - No KB endpoint configured, using enhanced mock response
2025-09-27 23:58:59,006 - kb_integration - INFO - No KB endpoint configured, using enhanced mock response
2025-09-28 00:00:14,751 - app - INFO - üîå API Shutdown
2025-09-28 00:00:26,464 - __main__ - INFO - === API Starting - Logging Config Applied ===
2025-09-28 00:00:26,657 - app - INFO - === API Starting - Logging Config Applied ===
2025-09-28 00:00:26,670 - app - INFO - üöÄ API Startup
2025-09-28 00:00:26,679 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-09-28 00:00:27,978 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-09-28 00:00:29,626 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-28 00:00:51,659 - app - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-09-28 00:00:51,660 - app - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-09-28 00:01:32,849 - kb_integration - INFO - No KB endpoint configured, using enhanced mock response
2025-09-28 00:01:41,090 - kb_integration - INFO - No KB endpoint configured, using enhanced mock response
2025-09-28 00:01:47,262 - kb_integration - INFO - No KB endpoint configured, using enhanced mock response
2025-09-28 00:41:35,909 - kb_integration - INFO - No KB endpoint configured, using enhanced mock response
2025-09-28 01:31:06,604 - kb_integration - INFO - No KB endpoint configured, using enhanced mock response
2025-09-28 01:32:15,727 - app - INFO - üîå API Shutdown
2025-09-28 01:32:35,416 - __main__ - INFO - === API Starting - Logging Config Applied ===
2025-09-28 01:32:35,612 - app - INFO - === API Starting - Logging Config Applied ===
2025-09-28 01:32:35,622 - app - INFO - üöÄ API Startup
2025-09-28 01:32:35,628 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-09-28 01:32:36,909 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-09-28 01:32:38,530 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-28 01:32:52,726 - app - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-09-28 01:32:52,726 - app - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-09-28 01:33:47,227 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:34:00,997 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:34:19,782 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:34:38,741 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:34:59,313 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:35:16,300 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:35:38,411 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:36:01,421 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:36:01,528 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:36:01,638 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:36:01,746 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:36:11,106 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:36:11,209 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:36:11,332 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 01:36:11,428 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:15:12,448 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:15:23,316 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:15:53,558 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:17:23,378 - app - INFO - === API Starting - Logging Config Applied ===
2025-09-28 02:17:23,649 - app - INFO - üöÄ API Startup
2025-09-28 02:17:23,661 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-09-28 02:17:25,076 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-09-28 02:17:26,711 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-28 02:17:57,508 - app - INFO - === API Starting - Logging Config Applied ===
2025-09-28 02:17:57,683 - app - INFO - üöÄ API Startup
2025-09-28 02:17:57,691 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-09-28 02:17:59,069 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-09-28 02:18:00,913 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-28 02:18:20,969 - app - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-09-28 02:18:20,970 - app - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-09-28 02:18:24,528 - app - INFO - === API Starting - Logging Config Applied ===
2025-09-28 02:18:24,716 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:18:24,822 - app - INFO - Generation skipped - generate_response: True, model: False, tokenizer: False
2025-09-28 02:18:44,570 - app - INFO - === API Starting - Logging Config Applied ===
2025-09-28 02:18:44,729 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-09-28 02:18:46,152 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-09-28 02:18:47,853 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-28 02:19:06,882 - app - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-09-28 02:19:06,883 - app - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-09-28 02:19:06,895 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:20:23,806 - __main__ - INFO - === API Starting - Logging Config Applied ===
2025-09-28 02:20:24,045 - app - INFO - === API Starting - Logging Config Applied ===
2025-09-28 02:20:24,054 - app - INFO - üöÄ API Startup
2025-09-28 02:20:24,062 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-09-28 02:20:25,340 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-09-28 02:20:26,982 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-28 02:20:41,647 - app - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-09-28 02:20:41,648 - app - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-09-28 02:21:21,186 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:21:21,291 - app - INFO - Starting response generation for query: Can you tell me about GenAI?...
2025-09-28 02:21:21,292 - app - INFO - Generated prompt: Question: Can you tell me about GenAI?
Knowledge Base Answer: I understand you're asking about 'Can ...
2025-09-28 02:21:21,294 - app - INFO - Starting model generation with max_new_tokens: 256
2025-09-28 02:21:42,905 - app - INFO - Full model response: Question: Can you tell me about GenAI?
Knowledge Base Answer: I understand you're asking about 'Can you tell me about GenAI?'. While I'm designed as a multilingual tokenization and language model syst...
2025-09-28 02:21:42,905 - app - INFO - Extracted generated response: I can provide some general information about GenAI, which refers to Artificial General Intelligence ...
2025-09-28 02:22:12,517 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:22:12,609 - app - INFO - Starting response generation for query: ‡§Ø‡•ã‡§ó ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?...
2025-09-28 02:22:12,609 - app - INFO - Generated prompt: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§Ø‡•ã‡§ó ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞: ‡§Ü‡§™‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® '‡§Ø‡•ã‡§ó ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?' ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ø‡§π ‡§ï‡§π ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç...
2025-09-28 02:22:12,612 - app - INFO - Starting model generation with max_new_tokens: 256
2025-09-28 02:22:35,074 - app - INFO - Full model response: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§Ø‡•ã‡§ó ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞: ‡§Ü‡§™‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® '‡§Ø‡•ã‡§ó ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?' ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ø‡§π ‡§ï‡§π ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§ï‡§ø ‡§Ø‡§π ‡§è‡§ï ‡§¨‡§π‡•Å‡§≠‡§æ‡§∑‡•Ä ‡§ü‡•ã‡§ï‡§®‡§æ‡§á‡§ú‡§º‡•á‡§∂‡§® ‡§î‡§∞ ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä ‡§ï‡§æ ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§π‡•à‡•§ ‡§Ö‡§ß‡§ø‡§ï ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö...
2025-09-28 02:22:35,076 - app - INFO - Extracted generated response: "‡§Ø‡•ã‡§ó" ‡§è‡§ï ‡§ê‡§∏‡§æ ‡§∂‡§¨‡•ç‡§¶ ‡§π‡•à ‡§ú‡§ø‡§∏‡§ï‡§æ ‡§Ö‡§∞‡•ç‡§• ‡§π‡•à "‡§Ø‡•ã‡§ó" ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§∂‡§¨‡•ç‡§¶‡•§ ‡§Ø‡•ã‡§ó ‡§è‡§ï ‡§™‡•ç‡§∞‡§æ‡§ö‡•Ä‡§® ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§¶‡§∞‡•ç‡§∂‡§®, ‡§µ‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§Æ...
2025-09-28 02:23:00,966 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:23:01,076 - app - INFO - Starting response generation for query: ‡§≠‡§æ‡§∞‡§§‡§æ‡§ö‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡§æ‡§Ø ‡§Ü‡§π‡•á?...
2025-09-28 02:23:01,076 - app - INFO - Generated prompt: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§≠‡§æ‡§∞‡§§‡§æ‡§ö‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡§æ‡§Ø ‡§Ü‡§π‡•á?
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞‡§æ‡§ö‡•á ‡§â‡§§‡•ç‡§§‡§∞: ‡§π‡§æ ‡§è‡§ï ‡§≠‡•Ç‡§ó‡•ã‡§≤‡§æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§Ü‡§π‡•á‡•§ ‡§Æ‡•Ä ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§∏‡§æ‡§Ç‡§ó‡•Ç ‡§∂...
2025-09-28 02:23:01,078 - app - INFO - Starting model generation with max_new_tokens: 256
2025-09-28 02:23:23,028 - app - INFO - Full model response: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§≠‡§æ‡§∞‡§§‡§æ‡§ö‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡§æ‡§Ø ‡§Ü‡§π‡•á?
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞‡§æ‡§ö‡•á ‡§â‡§§‡•ç‡§§‡§∞: ‡§π‡§æ ‡§è‡§ï ‡§≠‡•Ç‡§ó‡•ã‡§≤‡§æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§Ü‡§π‡•á‡•§ ‡§Æ‡•Ä ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§∏‡§æ‡§Ç‡§ó‡•Ç ‡§∂‡§ï‡§§‡•ã ‡§ï‡•Ä ‡§≠‡§æ‡§∞‡§§‡§æ‡§ö‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§®‡§µ‡•Ä ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§Ü‡§π‡•á‡•§ ‡§§‡§•‡§æ‡§™‡§ø, ‡§π‡•Ä ‡§è‡§ï ‡§Æ‡•â‡§ï ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§Ü‡§π‡•á - ‡§Ö‡§ö‡•Ç‡§ï ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä‡§∏‡§æ‡§†‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§∏‡§®‡•Ä‡§Ø ...
2025-09-28 02:23:23,029 - app - INFO - Extracted generated response: ‡§≠‡§æ‡§∞‡§§‡§æ‡§ö‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§§‡•Ä ‡§¶‡•á‡§∂‡§æ‡§ö‡•Ä ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§ï‡•Ä‡§Ø ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞‡§ø‡§§ ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§Ö‡§∏‡§≤‡•á‡§≤‡•Ä ‡§∂‡§π‡§∞. ‡§≠‡§æ‡§∞‡§§‡§æ‡§ö‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§®‡§µ‡•Ä ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§Ü...
2025-09-28 02:23:37,201 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:23:37,310 - app - INFO - Starting response generation for query: What is tokenization?...
2025-09-28 02:23:37,311 - app - INFO - Generated prompt: Question: What is tokenization?
Knowledge Base Answer: I understand you're asking about 'What is tok...
2025-09-28 02:23:37,312 - app - INFO - Starting model generation with max_new_tokens: 256
2025-09-28 02:23:59,789 - app - INFO - Full model response: Question: What is tokenization?
Knowledge Base Answer: I understand you're asking about 'What is tokenization?'. While I'm designed as a multilingual tokenization and language model system, I can prov...
2025-09-28 02:23:59,790 - app - INFO - Extracted generated response: u-n-b-r-e-a-k-a-b-l-e. This allows the model to better understand the individual components of the w...
2025-09-28 02:24:18,185 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:24:18,293 - app - INFO - Starting response generation for query: Can you tell me about GenAI?...
2025-09-28 02:24:18,293 - app - INFO - Generated prompt: Question: Can you tell me about GenAI?
Knowledge Base Answer: I understand you're asking about 'Can ...
2025-09-28 02:24:18,296 - app - INFO - Starting model generation with max_new_tokens: 256
2025-09-28 02:24:30,447 - app - INFO - Full model response: Question: Can you tell me about GenAI?
Knowledge Base Answer: I understand you're asking about 'Can you tell me about GenAI?'. While I'm designed as a multilingual tokenization and language model syst...
2025-09-28 02:24:30,448 - app - INFO - Extracted generated response: The provided 'Improved Answer' is based on the knowledge of the AI and Machine Learning Knowledge Ba...
2025-09-28 02:24:46,316 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:24:46,422 - app - INFO - Starting response generation for query: Explain the concept of machine learning...
2025-09-28 02:24:46,422 - app - INFO - Generated prompt: Question: Explain the concept of machine learning
Knowledge Base Answer: Hello! I'm here to help you...
2025-09-28 02:24:46,423 - app - INFO - Starting model generation with max_new_tokens: 256
2025-09-28 02:25:07,725 - app - INFO - Full model response: Question: Explain the concept of machine learning
Knowledge Base Answer: Hello! I'm here to help you with your questions. I can communicate in Hindi, Sanskrit, Marathi, and English.

(Source: Multilin...
2025-09-28 02:25:07,726 - app - INFO - Extracted generated response: ‡§π‡•á‡§≤‡•ã! ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§π‡•Ç‡§Å‡•§ ‡§Æ‡•à‡§Ç ‡§π...
2025-09-28 02:26:27,190 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:26:27,294 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:26:27,401 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:26:27,510 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:27:06,738 - app - INFO - üîå API Shutdown
2025-09-28 02:30:37,378 - app - INFO - === API Starting - Logging Config Applied ===
2025-09-28 02:30:37,616 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-09-28 02:30:39,486 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-09-28 02:30:41,479 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-28 02:31:04,711 - app - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-09-28 02:31:04,713 - app - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-09-28 02:31:04,731 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:31:33,854 - __main__ - INFO - === API Starting - Logging Config Applied ===
2025-09-28 02:31:34,078 - app - INFO - === API Starting - Logging Config Applied ===
2025-09-28 02:31:34,090 - app - INFO - üöÄ API Startup
2025-09-28 02:31:34,100 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-09-28 02:31:35,332 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-09-28 02:31:36,929 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-28 02:31:52,405 - app - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-09-28 02:31:52,407 - app - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-09-28 02:32:14,038 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:32:14,147 - app - INFO - Starting response generation for language switching query: Tell me about India...
2025-09-28 02:32:14,147 - app - INFO - Generated prompt for english: Question: Tell me about India
Knowledge Base Answer: I understand you're asking about 'Tell me about...
2025-09-28 02:32:14,219 - app - INFO - Starting model generation for english with max_new_tokens: 256
2025-09-28 02:32:36,379 - app - INFO - Full model response for english: Question: Tell me about India
Knowledge Base Answer: I understand you're asking about 'Tell me about India'. While I'm designed as a multilingual tokenization and language model system, I can provide ...
2025-09-28 02:32:36,380 - app - INFO - Extracted generated response for english: Government of India, Encyclopedia Britannica, and other reliable sources)

Please note that this is ...
2025-09-28 02:32:36,381 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:32:36,482 - app - INFO - Starting response generation for language switching query: Please explain in English...
2025-09-28 02:32:36,483 - app - INFO - Generated prompt for english: Question: Please explain in English
Knowledge Base Answer: This appears to be an educational questio...
2025-09-28 02:32:36,489 - app - INFO - Starting model generation for english with max_new_tokens: 256
2025-09-28 02:32:56,571 - app - INFO - Full model response for english: Question: Please explain in English
Knowledge Base Answer: This appears to be an educational question. I can assist you with topics related to multilingual language processing, tokenization, and natur...
2025-09-28 02:32:56,571 - app - INFO - Extracted generated response for english: I can explain the concepts of multilingual tokenization and its application in natural language proc...
2025-09-28 02:32:56,573 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:32:56,677 - app - INFO - Starting response generation for language switching query: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è...
2025-09-28 02:32:56,677 - app - INFO - Generated prompt for hindi: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞: ‡§Ü‡§™‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® '‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è' ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ø‡§π ‡§ï‡§π ‡§∏‡§ï...
2025-09-28 02:32:56,685 - app - INFO - Starting model generation for hindi with max_new_tokens: 256
2025-09-28 02:33:17,008 - app - INFO - Full model response for hindi: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞: ‡§Ü‡§™‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® '‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è' ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ø‡§π ‡§ï‡§π ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§ï‡§ø ‡§Ø‡§π ‡§è‡§ï ‡§¨‡§π‡•Å‡§≠‡§æ‡§∑‡•Ä ‡§ü‡•ã‡§ï‡§®‡§æ‡§á‡§ú‡§º‡•á‡§∂‡§® ‡§î‡§∞ ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä ‡§ï‡§æ ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§π‡•à‡•§ ‡§Ö‡§ß‡§ø‡§ï ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï...
2025-09-28 02:33:17,008 - app - INFO - Extracted generated response for hindi: ‡§Æ‡•à‡§Ç ‡§∏‡§Æ‡§ù‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§ï‡§ø ‡§Ü‡§™ ‡§è‡§ï ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§â‡§§‡•ç‡§§‡§∞ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç‡•§ '‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è' ‡§µ‡§æ‡§ï‡•ç‡§Ø‡§æ‡§Ç‡§∂ ‡§ï‡•á ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•ã ‡§¨...
2025-09-28 02:33:17,010 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-09-28 02:33:17,111 - app - INFO - Starting response generation for language switching query: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ...
2025-09-28 02:33:17,111 - app - INFO - Generated prompt for hindi: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞: ‡§Ü‡§™‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® '‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ' ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ø‡§π ‡§ï‡§π ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç...
2025-09-28 02:33:17,113 - app - INFO - Starting model generation for hindi with max_new_tokens: 256
2025-09-28 02:33:37,499 - app - INFO - Full model response for hindi: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞: ‡§Ü‡§™‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® '‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ' ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ø‡§π ‡§ï‡§π ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§ï‡§ø ‡§Ø‡§π ‡§è‡§ï ‡§¨‡§π‡•Å‡§≠‡§æ‡§∑‡•Ä ‡§ü‡•ã‡§ï‡§®‡§æ‡§á‡§ú‡§º‡•á‡§∂‡§® ‡§î‡§∞ ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä ‡§ï‡§æ ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§π‡•à‡•§ ‡§Ö‡§ß‡§ø‡§ï ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö...
2025-09-28 02:33:37,499 - app - INFO - Extracted generated response for hindi: '‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ' ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ø‡§π ‡§ï‡§π ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§ï‡§ø ‡§Ø‡§π ‡§è‡§ï ‡§ü‡•ã‡§ï‡§®‡§æ‡§á‡§ú‡§º‡•á‡§∂‡§® ‡§î‡§∞ ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä ‡§ï‡§æ ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§π...
2025-09-28 02:35:02,638 - app - INFO - üîå API Shutdown
2025-09-30 22:46:58,475 - __main__ - INFO - === API Starting - Logging Config Applied ===
2025-09-30 22:46:58,747 - app - INFO - === API Starting - Logging Config Applied ===
2025-09-30 22:46:58,760 - app - INFO - üöÄ API Startup
2025-09-30 22:46:58,768 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-09-30 22:47:00,637 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-09-30 22:47:02,532 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-30 22:47:14,576 - app - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-09-30 22:47:14,579 - app - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-10-01 13:39:12,319 - __main__ - INFO - === API Starting - Logging Config Applied ===
2025-10-01 13:39:12,489 - app - INFO - === API Starting - Logging Config Applied ===
2025-10-01 13:39:12,509 - app - INFO - üöÄ API Startup
2025-10-01 13:39:12,514 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-10-01 13:39:13,502 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-10-01 13:39:14,603 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-01 13:39:23,455 - app - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-10-01 13:39:23,455 - app - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-10-01 13:41:34,103 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-01 13:41:34,201 - app - INFO - Starting response generation for query: What is the capital of India?...
2025-10-01 13:41:34,201 - app - INFO - Generated prompt: Question: What is the capital of India?
Knowledge Base Answer: This is a geography-related question....
2025-10-01 13:41:34,204 - app - INFO - Starting model generation with max_new_tokens: 256
2025-10-01 13:41:53,049 - app - INFO - Full model response: Question: What is the capital of India?
Knowledge Base Answer: This is a geography-related question. I can tell you that the capital of India is New Delhi. However, this is a mock response - for accur...
2025-10-01 13:41:53,049 - app - INFO - Extracted generated response: New Delhi is home to many iconic landmarks, including the...
2025-10-01 13:42:29,075 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-01 13:42:29,166 - app - INFO - Starting response generation for query: ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?...
2025-10-01 13:42:29,167 - app - INFO - Generated prompt: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞: ‡§Ø‡§π ‡§è‡§ï ‡§≠‡•Ç‡§ó‡•ã‡§≤ ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§π‡•à‡•§ ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•ã ‡§¨‡§§‡§æ ‡§∏‡§ï‡§§...
2025-10-01 13:42:29,170 - app - INFO - Starting model generation with max_new_tokens: 256
2025-10-01 13:42:47,609 - app - INFO - Full model response: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞: ‡§Ø‡§π ‡§è‡§ï ‡§≠‡•Ç‡§ó‡•ã‡§≤ ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§π‡•à‡•§ ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•ã ‡§¨‡§§‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§ï‡§ø ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§®‡§à ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§π‡•à‡•§ ‡§π‡§æ‡§≤‡§æ‡§Ç‡§ï‡§ø, ‡§Ø‡§π ‡§è‡§ï ‡§Æ‡•â‡§ï ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§π‡•à - ‡§∏‡§ü‡•Ä‡§ï ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡§ø‡§∂‡•ç‡§µ...
2025-10-01 13:42:47,609 - app - INFO - Extracted generated response: ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§∏‡•ç‡§∞‡•ã‡§§‡•ã‡§Ç ‡§ï‡•Ä ‡§ú‡§æ‡§Ç‡§ö ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§Ü‡§™...
2025-10-01 13:43:22,018 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-01 13:48:28,482 - __main__ - INFO - === API Starting - Logging Config Applied ===
2025-10-01 13:48:28,599 - app - INFO - === API Starting - Logging Config Applied ===
2025-10-01 13:48:28,609 - app - INFO - üöÄ API Startup
2025-10-01 13:48:28,616 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-10-01 13:48:29,514 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-10-01 13:48:30,445 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-01 13:48:39,410 - app - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-10-01 13:48:39,411 - app - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-10-01 13:49:01,614 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-01 13:49:01,726 - app - INFO - Starting response generation for query: What is the capital of India?...
2025-10-01 13:49:01,726 - app - INFO - Generated prompt: Question: What is the capital of India?
Knowledge Base Answer: This is a geography-related question....
2025-10-01 13:49:01,746 - app - INFO - Starting model generation with max_new_tokens: 256
2025-10-01 13:49:38,634 - app - INFO - Full model response: Question: What is the capital of India?
Knowledge Base Answer: This is a geography-related question. I can tell you that the capital of India is New Delhi. However, this is a mock response - for accur...
2025-10-01 13:49:38,634 - app - INFO - Extracted generated response: The improved response provides more context, clarity, and helpfulness to the user, while the mock re...
2025-10-01 13:51:41,304 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-01 13:51:41,411 - app - INFO - Starting response generation for language switching query: Tell me about India...
2025-10-01 13:51:41,412 - app - INFO - Generated prompt for english: Question: Tell me about India
Knowledge Base Answer: I understand you're asking about 'Tell me about...
2025-10-01 13:51:41,415 - app - INFO - Starting model generation for english with max_new_tokens: 256
2025-10-01 13:52:12,101 - app - INFO - Full model response for english: Question: Tell me about India
Knowledge Base Answer: I understand you're asking about 'Tell me about India'. While I'm designed as a multilingual tokenization and language model system, I can provide ...
2025-10-01 13:52:12,103 - app - INFO - Extracted generated response for english: I can give you some general information about India. India is a vast and diverse country, with a ric...
2025-10-01 13:52:12,104 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-01 13:52:12,207 - app - INFO - Starting response generation for language switching query: Please explain in English...
2025-10-01 13:52:12,209 - app - INFO - Generated prompt for english: Question: Please explain in English
Knowledge Base Answer: This appears to be an educational questio...
2025-10-01 13:52:12,212 - app - INFO - Starting model generation for english with max_new_tokens: 256
2025-10-01 13:52:38,104 - app - INFO - Full model response for english: Question: Please explain in English
Knowledge Base Answer: This appears to be an educational question. I can assist you with topics related to multilingual language processing, tokenization, and natur...
2025-10-01 13:52:38,105 - app - INFO - Extracted generated response for english: This approach involves combining script-based and language-based tokenization. For example, a tokeni...
2025-10-01 13:52:38,106 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-01 13:52:38,209 - app - INFO - Starting response generation for language switching query: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è...
2025-10-01 13:52:38,210 - app - INFO - Generated prompt for hindi: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞: ‡§Ü‡§™‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® '‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è' ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ø‡§π ‡§ï‡§π ‡§∏‡§ï...
2025-10-01 13:52:38,213 - app - INFO - Starting model generation for hindi with max_new_tokens: 256
2025-10-01 13:53:02,381 - app - INFO - Full model response for hindi: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞: ‡§Ü‡§™‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® '‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è' ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ø‡§π ‡§ï‡§π ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§ï‡§ø ‡§Ø‡§π ‡§è‡§ï ‡§¨‡§π‡•Å‡§≠‡§æ‡§∑‡•Ä ‡§ü‡•ã‡§ï‡§®‡§æ‡§á‡§ú‡§º‡•á‡§∂‡§® ‡§î‡§∞ ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä ‡§ï‡§æ ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§π‡•à‡•§ ‡§Ö‡§ß‡§ø‡§ï ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï...
2025-10-01 13:53:02,381 - app - INFO - Extracted generated response for hindi: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§µ‡•ç‡§Ø‡§æ‡§ï‡§∞‡§£ ‡§®‡§ø‡§Ø‡§Æ‡•ã‡§Ç ‡§ï‡§æ ‡§Ö‡§®‡•Å‡§∏‡§∞‡§£ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§æ‡§ï‡•ç‡§Ø‡§æ‡§Ç‡§∂)
‡§Ø‡§¶‡§ø ‡§Ü‡§™‡§ï‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§ï‡§ø‡§∏‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§µ‡§ø‡§∑‡§Ø ‡§∏‡•á ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§π‡•à ‡§§‡•ã ...
2025-10-01 13:53:02,382 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-01 13:53:02,493 - app - INFO - Starting response generation for language switching query: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ...
2025-10-01 13:53:02,494 - app - INFO - Generated prompt for hindi: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞: ‡§Ü‡§™‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® '‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ' ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ø‡§π ‡§ï‡§π ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç...
2025-10-01 13:53:02,496 - app - INFO - Starting model generation for hindi with max_new_tokens: 256
2025-10-01 13:53:18,787 - app - INFO - Full model response for hindi: ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ
‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞: ‡§Ü‡§™‡§ï‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® '‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ' ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ø‡§π ‡§ï‡§π ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§ï‡§ø ‡§Ø‡§π ‡§è‡§ï ‡§¨‡§π‡•Å‡§≠‡§æ‡§∑‡•Ä ‡§ü‡•ã‡§ï‡§®‡§æ‡§á‡§ú‡§º‡•á‡§∂‡§® ‡§î‡§∞ ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•â‡§°‡§≤ ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä ‡§ï‡§æ ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§π‡•à‡•§ ‡§Ö‡§ß‡§ø‡§ï ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö...
2025-10-01 13:53:18,788 - app - INFO - Extracted generated response for hindi: ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è)
‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§µ‡§ø‡§∏‡•ç‡§§‡§æ‡§∞ ‡§∏‡•á ‡§™‡•Ç‡§õ‡•á‡§Ç, ‡§§‡§æ‡§ï‡§ø ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§™‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§Ø‡•Å‡§ï‡•ç‡§§...
2025-10-01 16:00:47,136 - __main__ - INFO - === API Starting - Logging Config Applied ===
2025-10-01 16:00:47,388 - app - INFO - === API Starting - Logging Config Applied ===
2025-10-01 16:00:47,401 - app - INFO - üöÄ API Startup
2025-10-01 16:00:47,409 - app - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-10-01 16:00:50,000 - app - INFO - üîß Using 4-bit quantization for faster inference
2025-10-01 16:00:51,528 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-01 16:01:04,699 - app - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-10-01 16:01:04,700 - app - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-10-01 16:03:11,186 - kb_integration - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-01 16:03:11,277 - app - INFO - Starting response generation for query: What is the capital of India?...
2025-10-01 16:03:11,278 - app - INFO - Generated prompt: Question: What is the capital of India?
Knowledge Base Answer: This is a geography-related question....
2025-10-01 16:03:11,281 - app - INFO - Starting model generation with max_new_tokens: 256
2025-10-01 16:04:34,433 - app - INFO - Full model response: Question: What is the capital of India?
Knowledge Base Answer: This is a geography-related question. I can tell you that the capital of India is New Delhi. However, this is a mock response - for accur...
2025-10-01 16:04:34,436 - app - INFO - Extracted generated response: The capital of India is indeed New Delhi. It has been the capital of India since 1911, when the Brit...
2025-10-03 21:57:59,176 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-03 21:58:04,328 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-03 21:58:04,489 - src.api.main - INFO - üöÄ API Startup
2025-10-03 21:58:04,500 - src.api.main - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-10-03 22:42:37,812 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-03 22:44:50,322 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-03 22:45:05,672 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-03 22:47:24,242 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-03 22:47:58,560 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-03 22:48:17,072 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-03 22:48:30,301 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-04 00:01:31,772 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-04 00:01:40,192 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-04 00:01:40,456 - src.api.main - INFO - üöÄ API Startup
2025-10-04 00:01:40,473 - src.api.main - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-10-04 00:01:42,021 - src.api.main - INFO - üîß Using 4-bit quantization for faster inference
2025-10-04 00:01:43,903 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-04 00:02:05,278 - src.api.main - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-10-04 00:02:05,280 - src.api.main - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-10-04 01:09:26,209 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:10:01,271 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:10:54,250 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:10:54,251 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:10:54,253 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:10:54,365 - src.api.main - INFO - Starting response generation for language switching query: Hello, how are you?...
2025-10-04 01:10:54,366 - src.api.main - INFO - Generated prompt for english: Question: Hello, how are you?
Knowledge Base Answer: Hello! I'm here to help you with your questions...
2025-10-04 01:10:54,380 - src.api.main - INFO - Starting model generation for english with max_new_tokens: 256
2025-10-04 01:11:19,773 - src.api.main - INFO - Full model response for english: Question: Hello, how are you?
Knowledge Base Answer: Hello! I'm here to help you with your questions. I can communicate in Hindi, Sanskrit, Marathi, and English.

(Source: Multilingual Tokenization AP...
2025-10-04 01:11:19,774 - src.api.main - INFO - Extracted generated response for english: Hello...
2025-10-04 01:11:19,776 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:11:40,626 - src.api.main - INFO - Starting response generation for language switching query: Please explain in English...
2025-10-04 01:11:40,626 - src.api.main - INFO - Generated prompt for english: Question: Please explain in English
Knowledge Base Answer: This appears to be an educational questio...
2025-10-04 01:11:40,632 - src.api.main - INFO - Starting model generation for english with max_new_tokens: 256
2025-10-04 01:12:05,604 - src.api.main - INFO - Full model response for english: Question: Please explain in English
Knowledge Base Answer: This appears to be an educational question. I can assist you with topics related to multilingual language processing, tokenization, and natur...
2025-10-04 01:12:05,604 - src.api.main - INFO - Extracted generated response for english: These methods use predefined rules to split text into tokens. However, these rules can be language-s...
2025-10-04 01:12:05,606 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:12:05,715 - src.api.main - INFO - Starting response generation for language switching query: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è...
2025-10-04 01:12:05,715 - src.api.main - INFO - Generated prompt for nepali: Question: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è
Knowledge Base Answer: I understand you're asking about '‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è'....
2025-10-04 01:12:05,721 - src.api.main - INFO - Starting model generation for nepali with max_new_tokens: 256
2025-10-04 01:12:31,084 - src.api.main - INFO - Full model response for nepali: Question: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è
Knowledge Base Answer: I understand you're asking about '‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è'. While I'm designed as a multilingual tokenization and language model system, I can provide some gen...
2025-10-04 01:12:31,084 - src.api.main - INFO - Extracted generated response for nepali: "Please tell me the topic and I'll be happy to help.")

If you're asking a question, please go ahead...
2025-10-04 01:12:31,087 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:12:31,189 - src.api.main - INFO - Starting response generation for language switching query: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ...
2025-10-04 01:12:31,189 - src.api.main - INFO - Generated prompt for nepali: Question: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ
Knowledge Base Answer: I understand you're asking about '‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ'. While...
2025-10-04 01:12:31,196 - src.api.main - INFO - Starting model generation for nepali with max_new_tokens: 256
2025-10-04 01:12:55,957 - src.api.main - INFO - Full model response for nepali: Question: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ
Knowledge Base Answer: I understand you're asking about '‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ'. While I'm designed as a multilingual tokenization and language model system, I can provide some general i...
2025-10-04 01:12:55,958 - src.api.main - INFO - Extracted generated response for nepali: ‡§π‡•ç‡§Ø‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§≤‡§æ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ‡§Ø‡§ö‡§æ ‡§Ö‡§∏‡§§‡•ã. ‡§™‡§£ ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§Æ‡•ç‡§π‡§£ '‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ' ‡§∏‡§æ‡§∞‡§ñ‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§Ç‡§ö‡•á ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ‡§®‡•á ‡§ï‡§ø‡§Ç...
2025-10-04 01:12:55,973 - src.api.main - INFO - üîå API Shutdown
2025-10-04 01:13:14,780 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-04 01:13:15,114 - src.api.main - INFO - üöÄ API Startup
2025-10-04 01:13:15,131 - src.api.main - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-10-04 01:13:16,699 - src.api.main - INFO - üîß Using 4-bit quantization for faster inference
2025-10-04 01:13:18,679 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-04 01:13:43,559 - src.api.main - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-10-04 01:13:43,560 - src.api.main - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-10-04 01:30:47,666 - src.api.main - INFO - üîå API Shutdown
2025-10-04 01:31:04,313 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-04 01:31:10,307 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-04 01:31:10,476 - src.api.main - INFO - üöÄ API Startup
2025-10-04 01:31:10,483 - src.api.main - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-10-04 01:31:11,682 - src.api.main - INFO - üîß Using 4-bit quantization for faster inference
2025-10-04 01:31:13,261 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-04 01:31:30,820 - src.api.main - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-10-04 01:31:30,820 - src.api.main - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-10-04 01:34:47,304 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:34:58,951 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:35:10,524 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:35:22,881 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:35:34,299 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:35:34,389 - src.api.main - INFO - Starting response generation for language switching query: Hello, how are you?...
2025-10-04 01:35:34,389 - src.api.main - INFO - Generated prompt for english: Question: Hello, how are you?
Knowledge Base Answer: Hello! I'm here to help you with your questions...
2025-10-04 01:35:34,391 - src.api.main - INFO - Starting model generation for english with max_new_tokens: 256
2025-10-04 01:35:59,229 - src.api.main - INFO - Full model response for english: Question: Hello, how are you?
Knowledge Base Answer: Hello! I'm here to help you with your questions. I can communicate in Hindi, Sanskrit, Marathi, and English.

(Source: Multilingual Tokenization AP...
2025-10-04 01:35:59,230 - src.api.main - INFO - Extracted generated response for english: 1. Ack...
2025-10-04 01:35:59,231 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:35:59,332 - src.api.main - INFO - Starting response generation for language switching query: Please explain in English...
2025-10-04 01:35:59,333 - src.api.main - INFO - Generated prompt for english: Question: Please explain in English
Knowledge Base Answer: This appears to be an educational questio...
2025-10-04 01:35:59,337 - src.api.main - INFO - Starting model generation for english with max_new_tokens: 256
2025-10-04 01:36:23,656 - src.api.main - INFO - Full model response for english: Question: Please explain in English
Knowledge Base Answer: This appears to be an educational question. I can assist you with topics related to multilingual language processing, tokenization, and natur...
2025-10-04 01:36:23,656 - src.api.main - INFO - Extracted generated response for english: After tokenization, the system must perform post-processing and normalization to ensure that the tok...
2025-10-04 01:36:23,657 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:36:23,763 - src.api.main - INFO - Starting response generation for language switching query: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è...
2025-10-04 01:36:23,763 - src.api.main - INFO - Generated prompt for nepali: Question: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è
Knowledge Base Answer: I understand you're asking about '‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è'....
2025-10-04 01:36:23,766 - src.api.main - INFO - Starting model generation for nepali with max_new_tokens: 256
2025-10-04 01:36:47,685 - src.api.main - INFO - Full model response for nepali: Question: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è
Knowledge Base Answer: I understand you're asking about '‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è'. While I'm designed as a multilingual tokenization and language model system, I can provide some gen...
2025-10-04 01:36:47,685 - src.api.main - INFO - Extracted generated response for nepali: 1. ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§è‡§ï ‡§Æ‡•å‡§ñ‡§ø‡§ï ‡§Ø‡§æ ‡§≤‡§ø‡§ñ‡§ø‡§§ ‡§Ü‡§¶‡•á‡§∂ ‡§π‡•à ‡§ú‡•ã ‡§ï‡§ø‡§∏‡•Ä ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§µ‡§ø‡§∑‡§Ø ‡§™‡§∞ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ ‡§Ø...
2025-10-04 01:36:47,687 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 01:36:47,790 - src.api.main - INFO - Starting response generation for language switching query: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ...
2025-10-04 01:36:47,796 - src.api.main - INFO - Generated prompt for nepali: Question: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ
Knowledge Base Answer: I understand you're asking about '‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ'. While...
2025-10-04 01:36:47,800 - src.api.main - INFO - Starting model generation for nepali with max_new_tokens: 256
2025-10-04 01:37:13,251 - src.api.main - INFO - Full model response for nepali: Question: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ
Knowledge Base Answer: I understand you're asking about '‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ'. While I'm designed as a multilingual tokenization and language model system, I can provide some general i...
2025-10-04 01:37:13,251 - src.api.main - INFO - Extracted generated response for nepali: ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§∏‡§æ‡§π‡§ø‡§§‡•ç‡§Ø‡§æ‡§ö‡•á ‡§∏‡•ç‡§µ‡§∞‡•Ç‡§™ ‡§Ü‡§π‡•á ‡§Ü‡§£‡§ø ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§∏‡§æ‡§π‡§ø‡§§‡•ç‡§Ø‡§æ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§≤‡•á‡§ñ‡§®‡§æ‡§ö‡•Ä ‡§∏‡•Å‡§∞‡•Å‡§µ‡§æ‡§§ ‡§Ü‡§π‡•á.
‡§∏‡§Ç‡§¶‡§∞...
2025-10-04 01:59:49,612 - src.api.main - INFO - üîå API Shutdown
2025-10-04 02:00:08,640 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-04 02:00:08,950 - src.api.main - INFO - üöÄ API Startup
2025-10-04 02:00:08,961 - src.api.main - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-10-04 02:00:10,505 - src.api.main - INFO - üîß Using 4-bit quantization for faster inference
2025-10-04 02:00:12,568 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-04 02:00:38,668 - src.api.main - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-10-04 02:00:38,669 - src.api.main - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-10-04 02:00:38,673 - src.api.main - INFO - üîå API Shutdown
2025-10-04 02:00:59,047 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-04 02:00:59,303 - src.api.main - INFO - üöÄ API Startup
2025-10-04 02:00:59,312 - src.api.main - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-10-04 02:01:00,635 - src.api.main - INFO - üîß Using 4-bit quantization for faster inference
2025-10-04 02:01:02,541 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-04 02:01:24,138 - src.api.main - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-10-04 02:01:24,139 - src.api.main - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-10-04 02:01:24,143 - src.api.main - INFO - üîå API Shutdown
2025-10-04 02:01:39,515 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-04 02:01:39,779 - src.api.main - INFO - üöÄ API Startup
2025-10-04 02:01:39,790 - src.api.main - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multi_tokenizer.model
2025-10-04 02:01:41,248 - src.api.main - INFO - üîß Using 4-bit quantization for faster inference
2025-10-04 02:01:43,202 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-04 14:51:21,119 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-04 14:51:29,531 - src.api.main - INFO - === API Starting - Logging Config Applied ===
2025-10-04 14:51:29,721 - src.api.main - INFO - üöÄ API Startup
2025-10-04 14:51:29,732 - src.api.main - INFO - ‚úÖ SentencePiece tokenizer loaded from model/multilingual_tokenizer.model
2025-10-04 14:51:31,272 - src.api.main - INFO - üîß Using 4-bit quantization for faster inference
2025-10-04 14:51:33,132 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-04 14:51:49,503 - src.api.main - INFO - ‚úÖ Model loaded with 4-bit quantization
2025-10-04 14:51:49,503 - src.api.main - INFO - ‚úÖ Model loaded: AhinsaAI/ahinsa0.5-llama3.2-3B
2025-10-04 14:59:13,727 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 14:59:23,676 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 14:59:33,464 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 14:59:42,269 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 14:59:51,060 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 14:59:51,157 - src.api.main - INFO - Starting response generation for language switching query: Hello, how are you?...
2025-10-04 14:59:51,157 - src.api.main - INFO - Generated prompt for english: Question: Hello, how are you?
Knowledge Base Answer: Hello! I'm here to help you with your questions...
2025-10-04 14:59:51,160 - src.api.main - INFO - Starting model generation for english with max_new_tokens: 256
2025-10-04 15:00:09,079 - src.api.main - INFO - Full model response for english: Question: Hello, how are you?
Knowledge Base Answer: Hello! I'm here to help you with your questions. I can communicate in Hindi, Sanskrit, Marathi, and English.

(Source: Multilingual Tokenization AP...
2025-10-04 15:00:09,079 - src.api.main - INFO - Extracted generated response for english: Hello! I'm here to help you with your questions. I'm fluent in Hindi, Sanskrit, Marathi, and English...
2025-10-04 15:00:09,080 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 15:00:09,194 - src.api.main - INFO - Starting response generation for language switching query: Please explain in English...
2025-10-04 15:00:09,194 - src.api.main - INFO - Generated prompt for english: Question: Please explain in English
Knowledge Base Answer: This appears to be an educational questio...
2025-10-04 15:00:09,196 - src.api.main - INFO - Starting model generation for english with max_new_tokens: 256
2025-10-04 15:00:26,377 - src.api.main - INFO - Full model response for english: Question: Please explain in English
Knowledge Base Answer: This appears to be an educational question. I can assist you with topics related to multilingual language processing, tokenization, and natur...
2025-10-04 15:00:26,377 - src.api.main - INFO - Extracted generated response for english: Identifying named entities such as people, organizations, and locations...
2025-10-04 15:00:26,378 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 15:00:26,485 - src.api.main - INFO - Starting response generation for language switching query: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è...
2025-10-04 15:00:26,485 - src.api.main - INFO - Generated prompt for nepali: Question: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è
Knowledge Base Answer: I understand you're asking about '‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è'....
2025-10-04 15:00:26,487 - src.api.main - INFO - Starting model generation for nepali with max_new_tokens: 256
2025-10-04 15:00:33,420 - src.api.main - INFO - Full model response for nepali: Question: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è
Knowledge Base Answer: I understand you're asking about '‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§á‡§è'. While I'm designed as a multilingual tokenization and language model system, I can provide some gen...
2025-10-04 15:00:33,420 - src.api.main - INFO - Extracted generated response for nepali: The response is a simple acknowledgement and invitation to ask a question in Hindi, rather than prov...
2025-10-04 15:00:33,421 - src.services.knowledge_base - WARNING - KB endpoint http://127.0.0.1:8001 appears to point to self, using mock response to prevent loop
2025-10-04 15:00:33,536 - src.api.main - INFO - Starting response generation for language switching query: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ...
2025-10-04 15:00:33,536 - src.api.main - INFO - Generated prompt for nepali: Question: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ
Knowledge Base Answer: I understand you're asking about '‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ'. While...
2025-10-04 15:00:33,538 - src.api.main - INFO - Starting model generation for nepali with max_new_tokens: 256
2025-10-04 15:00:49,701 - src.api.main - INFO - Full model response for nepali: Question: ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ
Knowledge Base Answer: I understand you're asking about '‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ'. While I'm designed as a multilingual tokenization and language model system, I can provide some general i...
2025-10-04 15:00:49,701 - src.api.main - INFO - Extracted generated response for nepali: ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§π‡•ç‡§Ø‡§æ‡§ö‡§æ ‡§Ö‡§∞‡•ç‡§• ‡§Æ‡§∞‡§æ‡§†‡•Ä...
2025-10-04 15:05:58,715 - src.api.main - INFO - üîå API Shutdown

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Multilingual Fine-tuning Training Script for Google Colab\n",
        "\n",
        "This notebook fine-tunes a language model on multilingual data (Hindi, Sanskrit, Marathi, English) using LoRA/PEFT for efficient training.\n",
        "\n",
        "## Features:\n",
        "- ‚úÖ All dependencies included\n",
        "- ‚úÖ Sample data generation for demo\n",
        "- ‚úÖ Memory-optimized for Colab's GPU constraints\n",
        "- ‚úÖ Automatic GPU detection and configuration\n",
        "- ‚úÖ Progress tracking and logging\n",
        "- ‚úÖ LoRA/PEFT support for efficient training\n",
        "\n",
        "## Usage:\n",
        "1. Enable GPU in Colab (Runtime > Change runtime type > GPU)\n",
        "2. Run each cell in sequence\n",
        "3. **Note**: If prompted for W&B API key, the notebook has been configured to disable all external logging\n",
        "4. Download the fine-tuned model when complete\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install -q transformers datasets accelerate peft bitsandbytes\n",
        "%pip install -q sentencepiece langdetect\n",
        "\n",
        "# Verify installation\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries and setup\n",
        "import logging\n",
        "import os\n",
        "import gc\n",
        "import hashlib\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"AhinsaAI/ahinsa0.5-llama3.2-3B\"  # Change this to your preferred model\n",
        "OUTPUT_DIR = \"fine_tuned_model\"\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "WARMUP_STEPS = 100\n",
        "LEARNING_RATE = 5e-5\n",
        "MAX_LENGTH = 512\n",
        "USE_QUANTIZATION = True\n",
        "USE_PEFT = True\n",
        "\n",
        "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
        "print(f\"üìä Training Epochs: {EPOCHS}\")\n",
        "print(f\"üîß Quantization: {'Enabled' if USE_QUANTIZATION else 'Disabled'}\")\n",
        "print(f\"üîß PEFT/LoRA: {'Enabled' if USE_PEFT else 'Disabled'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory and run garbage collection\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    gc.collect()\n",
        "\n",
        "def check_gpu_usage():\n",
        "    \"\"\"Check and log GPU usage\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.cuda.current_device()\n",
        "        gpu_name = torch.cuda.get_device_name(device)\n",
        "        memory_allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
        "        memory_reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
        "        memory_total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
        "        \n",
        "        logger.info(f\"GPU: {gpu_name}\")\n",
        "        logger.info(f\"GPU Memory - Allocated: {memory_allocated:.2f} GB, Reserved: {memory_reserved:.2f} GB, Total: {memory_total:.2f} GB\")\n",
        "        return True\n",
        "    else:\n",
        "        logger.info(\"CUDA not available - using CPU\")\n",
        "        return False\n",
        "\n",
        "# Check GPU\n",
        "check_gpu_usage()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Data Configuration\n",
        "\n",
        "**IMPORTANT**: Make sure your Google Drive has the following folder structure:\n",
        "\n",
        "```\n",
        "Google Drive/\n",
        "‚îî‚îÄ‚îÄ Data/\n",
        "    ‚îú‚îÄ‚îÄ training/\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ hi_train.txt    (Hindi training data)\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ sa_train.txt    (Sanskrit training data)\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ mr_train.txt    (Marathi training data)\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ en_train.txt    (English training data)\n",
        "    ‚îî‚îÄ‚îÄ validation/\n",
        "        ‚îú‚îÄ‚îÄ hi_val.txt      (Hindi validation data)\n",
        "        ‚îú‚îÄ‚îÄ sa_val.txt      (Sanskrit validation data)\n",
        "        ‚îú‚îÄ‚îÄ mr_val.txt      (Marathi validation data)\n",
        "        ‚îî‚îÄ‚îÄ en_val.txt      (English validation data)\n",
        "```\n",
        "\n",
        "**If your files have different names**, you can modify the `corpus_files` dictionary in the next cell to match your actual file names.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive and setup data paths\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up data paths to your Google Drive Data folder\n",
        "DRIVE_DATA_PATH = \"/content/drive/MyDrive/Data\"  # Adjust this path if your folder is different\n",
        "TRAINING_DATA_PATH = os.path.join(DRIVE_DATA_PATH, \"training\")\n",
        "VALIDATION_DATA_PATH = os.path.join(DRIVE_DATA_PATH, \"validation\")\n",
        "\n",
        "print(f\"üîó Google Drive mounted successfully!\")\n",
        "print(f\"üìÅ Data path: {DRIVE_DATA_PATH}\")\n",
        "print(f\"üìÅ Training data path: {TRAINING_DATA_PATH}\")\n",
        "print(f\"üìÅ Validation data path: {VALIDATION_DATA_PATH}\")\n",
        "\n",
        "# Check if the paths exist\n",
        "if os.path.exists(TRAINING_DATA_PATH):\n",
        "    print(f\"‚úÖ Training data folder found!\")\n",
        "    train_files = os.listdir(TRAINING_DATA_PATH)\n",
        "    print(f\"üìÑ Training files: {train_files}\")\n",
        "else:\n",
        "    print(f\"‚ùå Training data folder not found at: {TRAINING_DATA_PATH}\")\n",
        "    print(\"Please check your Google Drive folder structure\")\n",
        "\n",
        "if os.path.exists(VALIDATION_DATA_PATH):\n",
        "    print(f\"‚úÖ Validation data folder found!\")\n",
        "    val_files = os.listdir(VALIDATION_DATA_PATH)\n",
        "    print(f\"üìÑ Validation files: {val_files}\")\n",
        "else:\n",
        "    print(f\"‚ùå Validation data folder not found at: {VALIDATION_DATA_PATH}\")\n",
        "    print(\"Please check your Google Drive folder structure\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customize file names if needed\n",
        "# If your files have different names, modify the dictionaries below\n",
        "\n",
        "# Training file names (modify these to match your actual file names)\n",
        "TRAINING_FILES = {\n",
        "    \"hindi\": \"hi_train.txt\",\n",
        "    \"sanskrit\": \"sa_train.txt\", \n",
        "    \"marathi\": \"mr_train.txt\",\n",
        "    \"english\": \"en_train.txt\"\n",
        "}\n",
        "\n",
        "# Validation file names (modify these to match your actual file names)\n",
        "VALIDATION_FILES = {\n",
        "    \"hindi\": \"hi_val.txt\",\n",
        "    \"sanskrit\": \"sa_val.txt\", \n",
        "    \"marathi\": \"mr_val.txt\",\n",
        "    \"english\": \"en_val.txt\"\n",
        "}\n",
        "\n",
        "print(\"üìã Current file configuration:\")\n",
        "print(\"Training files:\")\n",
        "for lang, filename in TRAINING_FILES.items():\n",
        "    print(f\"  {lang}: {filename}\")\n",
        "print(\"\\nValidation files:\")\n",
        "for lang, filename in VALIDATION_FILES.items():\n",
        "    print(f\"  {lang}: {filename}\")\n",
        "\n",
        "print(f\"\\nüîç If you need to change file names, modify the TRAINING_FILES and VALIDATION_FILES dictionaries above.\")\n",
        "print(f\"üìÅ Make sure your Google Drive Data folder has the same structure as shown in the previous cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Disable Weights & Biases and other logging integrations\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Disable wandb completely\n",
        "try:\n",
        "    import wandb\n",
        "    wandb.init(mode=\"disabled\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "print(\"‚úÖ Disabled Weights & Biases and other logging integrations\")\n",
        "print(\"üìä Training progress will be shown in console logs only\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional environment setup to prevent W&B prompts\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Try to disable wandb if it's available\n",
        "try:\n",
        "    import wandb\n",
        "    os.environ[\"WANDB_PROJECT\"] = \"\"\n",
        "    print(\"‚úÖ Weights & Biases disabled via environment variables\")\n",
        "except ImportError:\n",
        "    print(\"‚úÖ Weights & Biases not installed - no action needed\")\n",
        "\n",
        "print(\"üìä All external logging integrations disabled\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer with optimizations\n",
        "def load_model_and_tokenizer():\n",
        "    \"\"\"Load model and tokenizer with memory optimizations\"\"\"\n",
        "    \n",
        "    clear_gpu_memory()\n",
        "    \n",
        "    # Load tokenizer\n",
        "    logger.info(f\"Loading tokenizer from {MODEL_NAME}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    \n",
        "    # Configure quantization\n",
        "    quantization_config = None\n",
        "    if USE_QUANTIZATION and torch.cuda.is_available():\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            llm_int8_threshold=6.0,\n",
        "            llm_int8_has_fp16_weight=False,\n",
        "        )\n",
        "        logger.info(\"üîß Using 8-bit quantization\")\n",
        "    \n",
        "    # Load model\n",
        "    logger.info(f\"Loading model from {MODEL_NAME}\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "        low_cpu_mem_usage=True,\n",
        "        torch_dtype=torch.float16 if quantization_config else torch.float32,\n",
        "    )\n",
        "    \n",
        "    # Apply LoRA if enabled\n",
        "    if USE_PEFT:\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=16,\n",
        "            lora_alpha=32,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        )\n",
        "        \n",
        "        model = get_peft_model(model, lora_config)\n",
        "        logger.info(\"üîß Applied LoRA adapters\")\n",
        "        model.print_trainable_parameters()\n",
        "    \n",
        "    # Add padding token\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        logger.info(\"Added EOS token as padding token\")\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = load_model_and_tokenizer()\n",
        "check_gpu_usage()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training data from Google Drive\n",
        "def load_training_data():\n",
        "    \"\"\"Load and tokenize training data from Google Drive\"\"\"\n",
        "    \n",
        "    # Use the configurable file names\n",
        "    corpus_files = TRAINING_FILES\n",
        "    \n",
        "    # Load training data from Google Drive\n",
        "    train_texts = []\n",
        "    for lang, filename in corpus_files.items():\n",
        "        filepath = os.path.join(TRAINING_DATA_PATH, filename)\n",
        "        if os.path.exists(filepath):\n",
        "            logger.info(f\"Loading {lang} training data from {filepath}\")\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "                filtered_lines = [line.strip() for line in lines if len(line.strip()) > 10]\n",
        "                train_texts.extend(filtered_lines)\n",
        "                logger.info(f\"Loaded {len(filtered_lines)} {lang} training samples\")\n",
        "        else:\n",
        "            logger.warning(f\"Training file not found: {filepath}\")\n",
        "            logger.info(f\"Available files in training directory: {os.listdir(TRAINING_DATA_PATH) if os.path.exists(TRAINING_DATA_PATH) else 'Directory not found'}\")\n",
        "    \n",
        "    # Load validation data from Google Drive\n",
        "    eval_texts = []\n",
        "    val_files = VALIDATION_FILES\n",
        "    \n",
        "    for lang, filename in val_files.items():\n",
        "        filepath = os.path.join(VALIDATION_DATA_PATH, filename)\n",
        "        if os.path.exists(filepath):\n",
        "            logger.info(f\"Loading {lang} validation data from {filepath}\")\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "                filtered_lines = [line.strip() for line in lines if len(line.strip()) > 10]\n",
        "                eval_texts.extend(filtered_lines)\n",
        "                logger.info(f\"Loaded {len(filtered_lines)} {lang} validation samples\")\n",
        "        else:\n",
        "            logger.warning(f\"Validation file not found: {filepath}\")\n",
        "            logger.info(f\"Available files in validation directory: {os.listdir(VALIDATION_DATA_PATH) if os.path.exists(VALIDATION_DATA_PATH) else 'Directory not found'}\")\n",
        "    \n",
        "    if len(train_texts) == 0:\n",
        "        logger.error(\"‚ùå No training data loaded! Please check your file paths and names.\")\n",
        "        logger.info(\"Expected file structure:\")\n",
        "        logger.info(\"Google Drive/Data/training/\")\n",
        "        logger.info(\"  ‚îú‚îÄ‚îÄ hi_train.txt\")\n",
        "        logger.info(\"  ‚îú‚îÄ‚îÄ sa_train.txt\")\n",
        "        logger.info(\"  ‚îú‚îÄ‚îÄ mr_train.txt\")\n",
        "        logger.info(\"  ‚îî‚îÄ‚îÄ en_train.txt\")\n",
        "        logger.info(\"Google Drive/Data/validation/\")\n",
        "        logger.info(\"  ‚îú‚îÄ‚îÄ hi_val.txt\")\n",
        "        logger.info(\"  ‚îú‚îÄ‚îÄ sa_val.txt\")\n",
        "        logger.info(\"  ‚îú‚îÄ‚îÄ mr_val.txt\")\n",
        "        logger.info(\"  ‚îî‚îÄ‚îÄ en_val.txt\")\n",
        "        return None, None\n",
        "    \n",
        "    logger.info(f\"‚úÖ Total training samples: {len(train_texts)}\")\n",
        "    logger.info(f\"‚úÖ Total validation samples: {len(eval_texts)}\")\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
        "    eval_dataset = Dataset.from_dict({\"text\": eval_texts}) if eval_texts else None\n",
        "    \n",
        "    return train_dataset, eval_dataset\n",
        "\n",
        "# Load and tokenize data\n",
        "train_dataset, eval_dataset = load_training_data()\n",
        "\n",
        "# Check if data was loaded successfully\n",
        "if train_dataset is None or eval_dataset is None:\n",
        "    raise ValueError(\"‚ùå Failed to load training data. Please check your Google Drive folder structure and file names.\")\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"], \n",
        "        truncation=True, \n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=True,\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "logger.info(\"Tokenizing training dataset...\")\n",
        "tokenized_train = train_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "tokenized_eval = None\n",
        "if eval_dataset:\n",
        "    logger.info(\"Tokenizing validation dataset...\")\n",
        "    tokenized_eval = eval_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "logger.info(\"‚úÖ Data tokenization completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup training\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    pad_to_multiple_of=8,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    logging_steps=50,\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=False,\n",
        "    dataloader_drop_last=True,\n",
        "    dataloader_pin_memory=False,\n",
        "    report_to=None,\n",
        "    dataloader_num_workers=0,\n",
        "    save_total_limit=2,\n",
        "    max_grad_norm=1.0,\n",
        "    save_strategy=\"steps\" if USE_PEFT else \"epoch\",\n",
        "    eval_strategy=\"steps\" if USE_PEFT else \"no\",\n",
        "    load_best_model_at_end=True if USE_PEFT else False,\n",
        "    remove_unused_columns=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Add progress callback\n",
        "from transformers import TrainerCallback\n",
        "class ProgressCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % 10 == 0:\n",
        "            logger.info(f\"Training step {state.global_step}/{state.max_steps} - Loss: {state.log_history[-1].get('train_loss', 'N/A') if state.log_history else 'N/A'}\")\n",
        "\n",
        "trainer.add_callback(ProgressCallback())\n",
        "\n",
        "print(\"‚úÖ Training setup completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute training\n",
        "clear_gpu_memory()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    logger.info(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "logger.info(\"üöÄ Starting training...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    logger.info(\"‚úÖ Training completed successfully!\")\n",
        "except torch.cuda.OutOfMemoryError as e:\n",
        "    logger.error(f\"CUDA out of memory error: {e}\")\n",
        "    logger.info(\"Try reducing BATCH_SIZE further or MAX_LENGTH\")\n",
        "    clear_gpu_memory()\n",
        "    raise\n",
        "except Exception as e:\n",
        "    logger.error(f\"Training error: {e}\")\n",
        "    clear_gpu_memory()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and create download package\n",
        "logger.info(f\"Saving model to {OUTPUT_DIR}\")\n",
        "if USE_PEFT:\n",
        "    model.save_pretrained(OUTPUT_DIR)\n",
        "    logger.info(\"‚úÖ Saved LoRA adapters\")\n",
        "else:\n",
        "    trainer.save_model()\n",
        "\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Create zip file for download\n",
        "import zipfile\n",
        "zip_filename = f\"{OUTPUT_DIR}.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk(OUTPUT_DIR):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, OUTPUT_DIR)\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "logger.info(f\"‚úÖ Model saved and packaged as {zip_filename}\")\n",
        "\n",
        "# Download the model\n",
        "from google.colab import files\n",
        "print(f\"\\nüì• Download your trained model:\")\n",
        "files.download(zip_filename)\n",
        "\n",
        "print(f\"\\nüéâ Training completed! Model saved to: {OUTPUT_DIR}\")\n",
        "print(f\"üì¶ Download package: {zip_filename}\")\n",
        "print(\"ü§ñ You can now use this model for inference!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the fine-tuned model (Optional)\n",
        "def test_model():\n",
        "    \"\"\"Test the fine-tuned model with sample prompts\"\"\"\n",
        "    \n",
        "    # Test prompts in different languages\n",
        "    test_prompts = [\n",
        "        \"‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§π‡•Ç‡§Ç\",  # Hindi\n",
        "        \"‡§∏‡§∞‡•ç‡§µ‡•á ‡§≠‡§µ‡§®‡•ç‡§§‡•Å ‡§∏‡•Å‡§ñ‡§ø‡§®‡§É\",  # Sanskrit\n",
        "        \"‡§Æ‡•Ä ‡§è‡§ï ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Ø ‡§Ü‡§π‡•á\",  # Marathi\n",
        "        \"I am learning multiple languages\"  # English\n",
        "    ]\n",
        "    \n",
        "    print(\"üß™ Testing fine-tuned model with sample prompts:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for prompt in test_prompts:\n",
        "        print(f\"\\nüìù Prompt: {prompt}\")\n",
        "        \n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "        \n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode response\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"ü§ñ Response: {response}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Test the model\n",
        "test_model()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
